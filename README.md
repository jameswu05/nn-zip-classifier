# Mid-Level Neural Network for Digits Classification
A mid-level neural network implementation from scratch on the MNIST Zip-Digits dataset. Includes forward and backward propagation, adaptive activation function selection, training/validation split and testing, and various gradient descent techniques (VLR-GD, SGD, VLR-GD-ES, SGD-ES, VLR-GD-WD, SGD-WD).
The primary purpose of this project was to develop a strong foundation in the theory and implementation of neural networks and to compare the performance of different gradient descent algorithms on digit classification. Writing the codebase in C++ made me implement all of the math and algorithms manually from scratch. I utilized a modular design with a single layer class, a neural network driver class, and flexible user choice for between-layer activation. The results were plotted using Matplotlib and the Python frontend was called using std::system.
